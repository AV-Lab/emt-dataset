{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def generate_square_mask(dim_trg: int, dim_src: int, mask_type: str) -> torch.Tensor:\n",
    "    mask = torch.ones(dim_trg, dim_trg)* float('-inf')\n",
    "    if mask_type == \"src\":\n",
    "        mask = torch.triu(mask, diagonal=1)\n",
    "    elif mask_type == \"tgt\":\n",
    "        mask = torch.triu(mask, diagonal=1)\n",
    "        #mask = torch.tril(mask)\n",
    "    elif mask_type == \"memory\":\n",
    "        mask = torch.ones(dim_trg, dim_src)* float('-inf')\n",
    "        mask = torch.triu(mask, diagonal=1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def generate_square_mask(dim_trg: int, dim_src: int, mask_type: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Generate a square mask for transformer attention mechanisms.\n",
    "    \n",
    "    Args:\n",
    "        dim_trg (int): Target sequence length.\n",
    "        dim_src (int): Source sequence length.\n",
    "        mask_type (str): Type of mask to generate. Can be \"src\", \"tgt\", or \"memory\".\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: A mask tensor with `-inf` values to block specific positions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize a square matrix filled with -inf (default to a fully masked state)\n",
    "    mask = torch.ones(dim_trg, dim_trg) * float('-inf')\n",
    "\n",
    "    if mask_type == \"src\":\n",
    "        # Source mask (self-attention in the encoder)\n",
    "        # Creates an upper triangular matrix with -inf above the diagonal\n",
    "        # This allows each token to attend to itself and previous tokens\n",
    "        mask = torch.triu(mask, diagonal=1)\n",
    "\n",
    "    elif mask_type == \"tgt\":\n",
    "        # Target mask (self-attention in the decoder)\n",
    "        # Prevents the decoder from attending to future tokens (causal mask)\n",
    "        mask = torch.triu(mask, diagonal=1)\n",
    "\n",
    "    elif mask_type == \"memory\":\n",
    "        # Memory mask (cross-attention between encoder and decoder)\n",
    "        # Controls which encoder outputs the decoder can attend to\n",
    "        mask = torch.ones(dim_trg, dim_src) * float('-inf')\n",
    "        mask = torch.triu(mask, diagonal=1)  # Prevents attending to future positions\n",
    "\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_square_mask(dim_trg = 12 ,dim_src = 8, mask_type=\"tgt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 2., 2., 2.])\n",
      "tensor([1.4286, 0.0000, 1.4286, 1.4286, 1.4286])\n",
      "tensor([0., 2., 2., 2., 2.])\n",
      "tensor([0., 0., 2., 2., 2.])\n",
      "tensor([0., 0., 0., 2., 0.])\n",
      "tensor([0.0000, 1.4286, 1.4286, 1.4286, 1.4286])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "dropout_layer = nn.Dropout(p=0.5)\n",
    "dropout_layers = nn.Dropout(p=0.5)\n",
    "dropout_layersss = nn.Dropout(p=0.3)\n",
    "\n",
    "torch.manual_seed(42)  # Set seed\n",
    "x = torch.ones(5)\n",
    "print(dropout_layer(x))  # First output\n",
    "print(dropout_layersss(x))\n",
    "print(dropout_layers(x))\n",
    "\n",
    "\n",
    "torch.manual_seed(42)  # Reset seed\n",
    "print(dropout_layer(x))  # Second output (should match the first)\n",
    "print(dropout_layers(x))\n",
    "print(dropout_layersss(x))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
