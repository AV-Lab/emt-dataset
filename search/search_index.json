{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"EMT Dataset","text":"EMT Features Continuous Footage <p>Each video runs for 2.5 to 3 minutes, contributing to a total of 57 minutes of video in the dataset, covering over 150 kilometers.</p> Heterogenous Annotations <p>Five main categories of agents are annotated: pedestrians, vehicles, small motorized vehicles, motorbikes and cyclists.</p> Variety of Scenarios <p>Captures the unique road topologies and traffic congestion characteristics of the Arab Gulf Region. </p> Tracking Annotations <p>Bounding boxes with consistent tracking ids are provided for all road agents over the frames.</p> Intention Annotations <p>Intention labels are provided for all categories of agents indicating merging, keeping-lane, turning and crossing.</p> Prediction Annotations <p>Future locations are provided to reflect the agents' future trajectories for collision avoidance.</p>"},{"location":"emt_dataset/","title":"Download","text":""},{"location":"emt_dataset/#download-videos","title":"Download videos","text":"<p>Download Videos (zip archive)</p>"},{"location":"emt_dataset/#download-annotations","title":"Download Annotations","text":"<p>Download EMT annotations</p>"},{"location":"emt_dataset/#emt-benchmark-datasets","title":"EMT Benchmark Datasets","text":"Dataset Description  Your browser does not support the video tag.  Tracking Benchmark DatasetThe Tracking Dataset is designed to assess the performance of algorithms in consistently identifying and maintaining object tracking over time within the dynamic complexities of driving environments. It evaluates the motion of vehicles, pedestrians, cyclists, and motorbikes, captured from a forward-facing camera. It challenges tracking algorithms to perform reliably under diverse traffic conditions, such as high congestion levels, unpredictable pedestrian movements, and frequent lane changes.The dataset encompasses 8,806 unique tracking IDs, featuring: 8,076 vehicles, 568 pedestrians, 158 motorbikes and 14 cyclists. With an average tracking duration of 6.5 seconds, this benchmark sets a rigorous standard for evaluating tracking algorithms in real-world traffic scenarios.   Your browser does not support the video tag.  Trajectory Prediction BenchmarkThe Trajectory Prediction Benchmark focuses on testing the capability of forecasting models to predict the movement of agents in heterogeneous and dynamic traffic conditions. This includes interactions between vehicles and other road users at large intersections, roundabouts, and etc. It aims to challenge models in effectively generalizing across diverse scenarios.This dataset features 4,821 unique agents, consisting of 4,347 vehicles and 386 pedestrians. The dataset code contains generation script for varying the length of the past trajectories and prediction horizons, employing a shifting window approach, along with frame-centered evaluation.   Your browser does not support the video tag.  Intention Prediction BenchmarkThe Intention Prediction Benchmark is crafted to test the ability of autonomous systems to predict the future actions of surrounding traffic participants. By analyzing past trajectories, it enables models to anticipate future actions of surrounding agents.The dataset offers comprehensive annotations for vehicle maneuvers and pedestrian behaviors. For vehicles, the labeled maneuvers capture high-level intended actions that shape future trajectories, including:Turn left/right, Keep lane, Merge (left/right), Brake, Stop and Reverse. For pedestrians, the dataset provides four behavior categories: Waiting to cross, Crossing, Walking (on sidewalks or pavements), Stopping (e.g., waiting at bus stops). In total, the dataset comprises 4,921 sequences of agent trajectories with detailed intention labels, offering a rich resource for understanding and predicting complex behaviors in diverse traffic scenarios."},{"location":"emt_dataset/#github-repository","title":"Github Repository","text":"<p>All code here</p>"},{"location":"emt_dataset/#cite-us","title":"Cite Us","text":"<p>If you find the dataset and supplied code useful, please use the following citation:</p> <pre><code>@article{EMTdataset2025,\n      title={EMT: A Visual Multi-Task Benchmark Dataset for Autonomous Driving in the Arab Gulf Region}, \n      author={Nadya Abdel Madjid and Murad Mebrahtu and Abdelmoamen Nasser and Bilal Hassan and Naoufel Werghi and Jorge Dias and Majid Khonji},\n      year={2025},\n      eprint={2502.19260},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2502.19260}, \n}\n</code></pre>"}]}