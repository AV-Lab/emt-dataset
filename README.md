# EMT Dataset

<p align="center">
    <img src="assets/Multi_object_tracking_with _ground-truth.gif" width="1200px"/>
    <br>
    <i>Multi-object tracking with ground truth annotations</i>
</p>

## Table of Contents
- [Introduction](#introduction)
- [Features](#features)
- [Quick Start](#quick-start)
- [Dataset Structure](#dataset-structure)
- [Repository Structure](#repository-structure)
- [Benchmarking](#benchmarking)
- [Links](#links)
- [Contact](#contact)

## Introduction
EMT is a comprehensive dataset for autonomous driving research, containing 57 minutes of diverse urban traffic footage from the Gulf Region. The dataset provides rich semantic annotations across two agent categories: people (pedestrians and cyclists), vehicles (seven classes). Each video segment spans 2.5-3 minutes, capturing challenging real-world scenarios:

- **Dense Urban Traffic**: Complex multi-agent interactions in congested scenarios
- **Weather Variations**: Clear and rainy conditions
- **Visual Challenges**: High reflections from road surfaces and adverse weather combinations (rainy nights)

The dataset provides dense annotations for:
- **Detection & Tracking**: Multi-object tracking with consistent IDs
- **Trajectory Prediction**: Future motion paths and social interactions
- **Intention Prediction**: Behavior understanding in complex scenarios

Validated through benchmarking on state-of-the-art models across tracking, trajectory prediction, and intention prediction tasks, with corresponding ground truth annotations for each benchmark.


### Data Collection
| Aspect | Description |
|:-------|:------------|
| Duration | 57 minutes total footage |
| Segments | 2.5-3 minutes continuous recordings |
| FPS | 10fps for annotated frames |
| Agent Classes | 2 Person classes and 7 Vehicle classes|

### Agent Categories
1. **People**   
   - Pedestrians
   - Cyclists

2. **Vehicles**
   - Motorbike
   - Small motorised vehicle
   - Medium vehicle
   - Large vehicle
   - Car
   - Bus
   - Emergency vehicle

### Dataset Statistics
| Category | Count |
|----------|------------|
| Annotated Frames | 34,386 |
| Bounding Boxes | 626,634 |
| Unique Agents | 9,094 |
| Vehicle Instances | 7,857 |
| Pedestrian Instances | 568 |

| **Class** | **Description** | **Number of Bounding Boxes** | **Number of Agents** |
|-----------|----------------|------------------------------|----------------------|
| Pedestrian | An individual walking on foot. | 24,574 | 568 |
| Cyclist | Any bicycle or electric bike rider. | 594 | 14 |
| Motorbike | Includes motorcycles, bikes, and scooters with two or three wheels. | 11,294 | 159 |
| Car | Any standard automobile. | 429,705 | 6,559 |
| Small motorized vehicle | Motorized transport smaller than a car, such as mobility scooters and quad bikes. | 767 | 13 |
| Medium vehicle | Includes vehicles larger than a standard car, such as vans or tractors. | 51,257 | 741 |
| Large vehicle | Refers to vehicles larger than vans, such as lorries, typically with six or more wheels. | 37,757 | 579 |
| Bus | Covers all types of buses, including school buses, single-deck, double-deck. | 19,244 | 200 |
| Emergency vehicle | Emergency response units like ambulances, police cars, and fire trucks, distinguished by red and blue flashing lights. | 1,182 | 9 |
| **_Overall:_** | | **576,374** | **8,842** |



## Quick Start

1. Download dataset:
```bash
chmod +x download.sh
./download.sh
```

2. To confirm data stats run the following command:
```bash
# Statistics
python dataset_statistics.py
```

## Dataset Structure
To use base models, the dataset structure should be as follows:
```
emt-dataset/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ annotations/
â”‚   â”‚   â”œâ”€â”€ intention_annotations/    # Agent intention labels
â”‚   â”‚   â”œâ”€â”€ tracking_annotations/     # Multi-object tracking data
â”‚   â”‚   â”œâ”€â”€ prediction_annotations/   # Behavior prediction labels
â”‚   â”‚   â””â”€â”€ metadata.txt             # Dataset metadata
â”‚   â”‚â”€â”€ frames/                      # extracted frames(annotated frames only)
â”‚   â””â”€â”€ videos/                      # Raw video sequences
```
NB: Videos are not necessary unless you intend to use visual cues 

## Repository Structure
```
emt-dataset/
â”œâ”€â”€ data/    # Containts dataset files and annotations
â”œâ”€â”€ intention/    # Intention prediction scripts
â”œâ”€â”€ prediction/    # Trajectory prediction scripts
â”œâ”€â”€ tracking/    # Tracking scripts
â”œâ”€â”€ visualize/    # All visualization scripts, for tracking, prediction and intention
â”œâ”€â”€ constants.py    # Contains all constants utilized in repo, including labels for intention classes
â”œâ”€â”€ dataset_statistics.py    # Script for printing statistics reported in the paper
â”œâ”€â”€ utils.py    # Contains scripts for generating intention, prediction sttings including k-fold cross validation setting, as well as frames extraction related fuinctions 
```

## Visualization

3. To visualize detection data run the following command:
```bash
# visualize
cd visualize
python tracking_visualize.py
```

## Benchmarking
We benchmark the dataset for the following tasks:
- **Multi-class MOT Tracking**: Multi-object tracking with consistent IDs
- **Trajectory Prediction**: Future motion paths and social interactions
- **Intention Prediction**: Behavior understanding in complex scenarios

### Multi-class MOT Tracking

### Trajectory Prediction
Prediction package runs trajectory prediction models using LSTM, Graph Neural Networks (GNNs), and Transformer-based architectures. It supports training and evaluation modes, allowing users to load pre-trained models and process trajectory data with different settings.

```
â”œâ”€â”€ prediction/
â”‚   â”œâ”€â”€ dataloaders/
â”‚   â”‚   â”œâ”€â”€ seq_loader.py 
â”‚   â”‚   â”œâ”€â”€ frame_loader.py 
â”‚   â”œâ”€â”€ evaluation/  
â”‚   â”‚   â”œâ”€â”€ metric_tracker.py         
â”‚   â”‚   â”œâ”€â”€ distance_metrics.py             
â”‚   â”‚   â”œâ”€â”€ utils.py                
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ gat_temporal.py  
â”‚   â”‚   â”œâ”€â”€ gat.py  
â”‚   â”‚   â”œâ”€â”€ gcn_temporal.py  
â”‚   â”‚   â”œâ”€â”€ gcn.py  
â”‚   â”‚   â”œâ”€â”€ rnn.py  
â”‚   â”‚   â”œâ”€â”€ transformer_GMM.py  
â”‚   â”‚   â””â”€â”€ transformer.py 
|   â””â”€â”€ run.py
```

Switch to prediction folder (cd prediction) and follow the details for running trajectory prediction models [here](prediction/README.md)

### Intention Prediction
Intention package runs LSTM-based prediction model in autoregressive and vanilla settings. It supports training and evaluation modes, allowing users to load pre-trained models.

```
â”œâ”€â”€ intention/
â”‚   â”œâ”€â”€ dataloaders/
â”‚   â”‚   â”œâ”€â”€ seq_loader.py 
â”‚   â”œâ”€â”€ evaluation/ 
â”‚   â”‚   â”œâ”€â”€ classification_metrics.py           
â”‚   â”‚   â”œâ”€â”€ distance_metrics.py             
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ rnn_autoregressive.py 
â”‚   â”‚   â”œâ”€â”€ rnn_vanilla.py  
|   â””â”€â”€ run.py
```

Switch to intention folder (cd intention) and follow the details for running intention prediction models [here](intention/README.md)

#### Installation Requirements
Install dependencies using:
```bash
pip install -r requirements.txt
```

#### Notes
- Ensure GPU support (`cuda`) is available if running on a GPU.
- When evaluating a model, a valid checkpoint file must be specified.
- Normalization is recommended for better model performance.



## ğŸ”— Links
- Repository: [GitHub - AV-Lab/emt-dataset](https://github.com/AV-Lab/emt-dataset)
- Website: [EMT Dataset](https://avlab.io/emt-dataset/)

## ğŸ“ Citation
If you use the EMT dataset in your research, please cite our paper:

```
@article{EMTdataset2025,
      title={EMT: A Visual Multi-Task Benchmark Dataset for Autonomous Driving in the Arab Gulf Region}, 
      author={Nadya Abdel Madjid and Murad Mebrahtu and Abdelmoamen Nasser and Bilal Hassan and Naoufel Werghi and Jorge Dias and Majid Khonji},
      year={2025},
      eprint={2502.19260},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2502.19260}, 
}
```
## Contact
- Murad: murad.mebrahtu@ku.ac.ae
- Nadya: 100049370@ku.ac.ae
